import streamlit as st
import feedparser
import pandas as pd
import yfinance as yf
import requests
import urllib.parse
from datetime import datetime, timedelta
import re
import urllib3

# å¿½ç•¥ SSL è­¦å‘Š (æ”¾åœ¨æœ€ä¸Šæ–¹ç¢ºä¿ç”Ÿæ•ˆ)
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# --- é é¢è¨­å®š ---
st.set_page_config(
    page_title="å°è‚¡æˆ°æƒ…å®¤ (è‡ªå‹•ä¸»åŠ›ç‰ˆ)",
    page_icon="ğŸ¯",
    layout="wide",
)

# --- CSS ç¾åŒ– ---
st.markdown("""
    <style>
    .big-font { font-size:18px !important; }
    .stDataFrame { width: 100%; }
    </style>
    """, unsafe_allow_html=True)

# --- å®šç¾©è³‡æ–™çµæ§‹ ---
KEYWORD_GROUPS = {
    'ğŸ”¥ ç†±é–€': ['æ”¶è³¼', 'ä½µè³¼', 'å…¥è‚¡', 'ç¶“ç‡Ÿæ¬Š', 'è™•ç½®è‚¡', 'æ³¨æ„è‚¡', 'é‡è¨Š'],
    'ğŸ’° ç‡Ÿæ”¶': ['è¨‚å–®', 'å¤§å–®', 'æ€¥å–®', 'è½‰å–®', 'ç‡Ÿæ”¶æ–°é«˜', 'ç²åˆ©æ–°é«˜', 'ä¸‰ç‡ä¸‰å‡'],
    'ğŸ­ ç”¢æ¥­': ['æ¼²åƒ¹', 'å ±åƒ¹', 'æ“´ç”¢', 'æ–°å» ', 'ç¼ºè²¨', 'ä¾›ä¸æ‡‰æ±‚', 'è³‡æœ¬æ”¯å‡º'],
    'ğŸ“ˆ è¨Šè™Ÿ': ['æ³•èªª', 'åº«è—è‚¡', 'å¢è³‡', 'æ¸›è³‡', 'è‚¡åˆ©', 'æ®–åˆ©ç‡', 'å¡«æ¯'],
    'ğŸ¤– ç§‘æŠ€': ['AI', 'ä¼ºæœå™¨', 'CPO', 'æ•£ç†±', 'æ©Ÿå™¨äºº', 'CoWoS', 'å…ˆé€²å°è£', 'çŸ½å…‰å­', 'B100']
}

TARGET_SITES = [
    'site:news.cnyes.com', 'site:money.udn.com', 'site:tw.stock.yahoo.com', 
    'site:ctee.com.tw', 'site:bnext.com.tw', 'site:technews.tw'
]

# --- æ ¸å¿ƒåŠŸèƒ½å‡½æ•¸ ---

def fetch_broker_data(url):
    """
    çˆ¬å– MoneyDJ/åˆ¸å•†åˆ†é»ç¶²é çš„è²·è¶…æ’è¡Œ (å·²ä¿®å¾© SSL æ†‘è­‰å•é¡Œ)
    """
    try:
        # å½è£æˆä¸€èˆ¬ç€è¦½å™¨
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        
        # é—œéµä¿®æ”¹ï¼šverify=False (å¿½ç•¥æ†‘è­‰æª¢æŸ¥)
        response = requests.get(url, headers=headers, verify=False)
        
        # MoneyDJ é€šå¸¸ä½¿ç”¨ big5 ç·¨ç¢¼
        response.encoding = 'big5'
        
        # ä½¿ç”¨ Pandas è§£æ HTML è¡¨æ ¼
        dfs = pd.read_html(response.text)
        
        # å°‹æ‰¾åŒ…å«æ•¸æ“šçš„è¡¨æ ¼
        target_df = None
        for df in dfs:
            if any("è²·è¶…å¼µæ•¸" in str(col) for col in df.columns):
                target_df = df
                break
        
        if target_df is not None:
            stock_list = []
            for index, row in target_df.iterrows():
                row_str = str(row.values)
                # æŠ“å– 4ç¢¼æ•¸å­— (ç°¡å–®éæ¿¾)
                codes = re.findall(r'[1-9]\d{3}', row_str)
                if codes:
                    stock_list.append(codes[0])
            
            # å»é‡ä¸¦å–å‰ 20 å
            return list(set(stock_list))[:20]
            
        return []
    except Exception as e:
        st.error(f"çˆ¬å–å¤±æ•—ï¼ŒåŸå› ï¼š{e}")
        return []

def get_google_news_combined(time_str):
    """
    time_str: "1h" (ä¸€å°æ™‚å…§), "12h" (æ—©å ±), "1d" (å…¨å¤©)
    """
    all_news = []
    progress_bar = st.progress(0)
    total_groups = len(KEYWORD_GROUPS)
    
    for i, (group_name, keywords) in enumerate(KEYWORD_GROUPS.items()):
        kw_query = " OR ".join(keywords)
        site_query = " OR ".join(TARGET_SITES)
        full_query = f"({kw_query}) AND ({site_query}) when:{time_str}"
        encoded_query = urllib.parse.quote(full_query)
        rss_url = f"https://news.google.com/rss/search?q={encoded_query}&hl=zh-TW&gl=TW&ceid=TW:zh-Hant"
        
        try:
            feed = feedparser.parse(rss_url)
            for entry in feed.entries:
                existing = next((item for item in all_news if item['æ¨™é¡Œ'] == entry.title), None)
                if existing:
                    if group_name not in existing['æ¶‰åŠé¢å‘']:
                        existing['æ¶‰åŠé¢å‘'] += f", {group_name}"
                else:
                    all_news.append({
                        'æ¨™é¡Œ': entry.title,
                        'é€£çµ': entry.link,
                        'æ™‚é–“': entry.published if 'published' in entry else datetime.now().strftime("%H:%M"),
                        'æ¶‰åŠé¢å‘': group_name,
                        'timestamp': pd.to_datetime(entry.published) if 'published' in entry else datetime.now()
                    })
        except:
            pass
        progress_bar.progress((i + 1) / total_groups)
    
    progress_bar.empty()
    return all_news

def get_tech_analysis(ticker_list):
    if not ticker_list: return pd.DataFrame()
    data = []
    for code in ticker_list:
        try:
            stock = yf.Ticker(f"{code}.TW")
            hist = stock.history(period="3mo")
            if len(hist) > 20:
                price = hist['Close'].iloc[-1]
                ma5 = hist['Close'].rolling(5).mean().iloc[-1]
                ma20 = hist['Close'].rolling(20).mean().iloc[-1]
                ma60 = hist['Close'].rolling(60).mean().iloc[-1]
                vol = hist['Volume'].iloc[-1]
                vol_ma5 = hist['Volume'].rolling(5).mean().iloc[-1]
                
                # è¶¨å‹¢åˆ¤æ–·
                trend = "ç›¤æ•´"
                if price > ma5 > ma20 > ma60: trend = "ğŸ”¥ å¼·å‹¢å¤šé ­"
                elif price > ma20 and price > ma60: trend = "ğŸ“ˆ å¤šé ­ä¿®æ­£"
                elif price < ma20: trend = "â„ï¸ å¼±å‹¢/ç©ºé ­"
                
                data.append({
                    'ä»£ç¢¼': code,
                    'ç¾åƒ¹': round(price, 2),
                    'æœˆç·šä¹–é›¢%': round(((price - ma20)/ma20)*100, 2),
                    'æŠ€è¡“å‹æ…‹': trend,
                    'é‡èƒ½': "çˆ†é‡" if vol > vol_ma5 * 1.5 else "ç¸®é‡" if vol < vol_ma5 * 0.7 else "æº«å’Œ"
                })
        except: continue
    return pd.DataFrame(data)

# --- å´é‚Šæ¬„ ---
st.sidebar.title("ğŸ¯ å°è‚¡æˆ°æƒ…å®¤ v3")

# 1. æ™‚é–“æ¨¡å¼
time_mode = st.sidebar.radio(
    "æ™‚é–“æ¨¡å¼", 
    ["â˜€ï¸ æ—©å ± (08:45å‰)", "âš¡ ç›¤ä¸­ (å³æ™‚çªç™¼)", "ğŸŒ™ ç›¤å¾Œ (å…¨æ—¥ç¸½çµ)"]
)

if "æ—©å ±" in time_mode:
    search_period = "12h"
    st.sidebar.info("æœå°‹æ˜¨æ™šæ”¶ç›¤å¾Œ ~ é–‹ç›¤å‰çš„æ–°è")
elif "ç›¤ä¸­" in time_mode:
    search_period = "1h"
    st.sidebar.warning("æœå°‹éå» 1 å°æ™‚å…§çš„æœ€æ–°çªç™¼")
else:
    search_period = "1d"
    st.sidebar.success("æœå°‹ä»Šæ—¥å…¨å¤©ç›¤å¾Œç¸½æ•´ç†")

st.sidebar.markdown("---")

# 2. è‡ªå‹•æŠ“å–åˆ¸å•†åˆ†é»
st.sidebar.subheader("ğŸ•µï¸â€â™‚ï¸ ä¸»åŠ›åˆ†é»è¿½è¹¤")
default_url = "https://fubon-ebrokerdj.fbs.com.tw/z/zg/zgb/zgb0.djhtm?a=9200&b=9268"
broker_url = st.sidebar.text_input("è¼¸å…¥åˆ¸å•†åˆ†é»ç¶²å€ (MoneyDJ/å¯Œé‚¦)", value=default_url)

manual_tickers = st.sidebar.text_area("æˆ–æ‰‹å‹•è¼¸å…¥ä»£ç¢¼ (é€—è™Ÿåˆ†éš”)", "")

# 3. åŸ·è¡Œ
run = st.sidebar.button("ğŸš€ å•Ÿå‹•æƒæ", type="primary")

# --- ä¸»ç•«é¢ ---
st.title(f"{time_mode} æˆ°æƒ…çœ‹æ¿")

if run:
    target_tickers = []
    
    # æŠ“å–åˆ†é»
    if broker_url:
        with st.spinner("æ­£åœ¨æ½›å…¥åˆ¸å•†ç¶²é æŠ“å–ä¸»åŠ›è²·è¶…è‚¡..."):
            scraped_tickers = fetch_broker_data(broker_url)
            if scraped_tickers:
                st.toast(f"æˆåŠŸæŠ“å–åˆ° {len(scraped_tickers)} æª”ä¸»åŠ›è‚¡ï¼")
                target_tickers.extend(scraped_tickers)
            else:
                st.error("ç„¡æ³•å¾ç¶²å€æŠ“å–è³‡æ–™ï¼Œè«‹ç¢ºèªç¶²å€æ ¼å¼ã€‚")
    
    # åŠ ä¸Šæ‰‹å‹•è¼¸å…¥
    if manual_tickers:
        manual_list = re.findall(r'[1-9]\d{3}', manual_tickers)
        target_tickers.extend(manual_list)
        
    target_tickers = list(set(target_tickers))

    col1, col2 = st.columns([1, 1])

    with col1:
        st.subheader("ğŸ“Š ä¸»åŠ›é—œæ³¨è‚¡ x æŠ€è¡“é¢")
        if target_tickers:
            df_tech = get_tech_analysis(target_tickers)
            if not df_tech.empty:
                df_tech = df_tech.sort_values(by="æœˆç·šä¹–é›¢%", ascending=False)
                st.dataframe(
                    df_tech, 
                    hide_index=True, 
                    use_container_width=True,
                    column_config={
                        "æŠ€è¡“å‹æ…‹": st.column_config.TextColumn("å‹æ…‹", width="small"),
                    }
                )
            else:
                st.info("ç„¡æ³•å–å¾—è‚¡åƒ¹ï¼Œå¯èƒ½æ˜¯ç›¤ä¸­APIé™åˆ¶æˆ–ä»£ç¢¼éŒ¯èª¤ã€‚")
        else:
            st.info("å°šæœªè¼¸å…¥æˆ–æŠ“å–åˆ°è‚¡ç¥¨ä»£ç¢¼")

    with col2:
        st.subheader("ğŸ“° å¸‚å ´ç„¦é»æ–°è")
        with st.spinner(f"æœå°‹éå» {search_period} æ–°èä¸­..."):
            news_data = get_google_news_combined(search_period)
            
        if news_data:
            df_news = pd.DataFrame(news_data)
            df_news = df_news.sort_values(by='timestamp', ascending=False)
            
            st.dataframe(
                df_news[['æ™‚é–“', 'æ¶‰åŠé¢å‘', 'æ¨™é¡Œ', 'é€£çµ']],
                column_config={
                    "é€£çµ": st.column_config.LinkColumn("Go", display_text="é–±è®€"),
                    "æ¨™é¡Œ": st.column_config.TextColumn("æ¨™é¡Œ", width="medium"),
                },
                hide_index=True,
                use_container_width=True
            )
        else:
            st.info("è©²æ™‚æ®µå…§ç„¡ç¬¦åˆæ¢ä»¶çš„é‡è¦æ–°èã€‚")
else:
    st.info("ğŸ‘ˆ è«‹åœ¨å·¦å´è¨­å®šå¾Œï¼Œé»æ“Šã€Œå•Ÿå‹•æƒæã€")
